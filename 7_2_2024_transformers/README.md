# Transformers
We will revisit the parenthesization dataset one last time in this assignment and train a model to predict valid parenthesizations with >99% accuracy. Instead of using a single linear layer, we will train a new architecture called a **transformer** (the 'T' in 'ChatGPT'). Specifically, we will train an encoder-only transformer. The paper that sparked this new architecture is titled "Attention is All You Need" and there is a version annotated with PyTorch code [here](http://nlp.seas.harvard.edu/annotated-transformer/).

In this assignment, most of the code and data has been set up for you, so to start, all you need to do is train the model. Since the datasets are larger than previous datasets you've worked with, the provided code also makes checkpoints so that you can resume training even if the program is interrupted.

After training and obtaining a model with high accuracy, you will write code to attempt to interpret the model. Unfortunately, transformer architectures are significantly more complicated than the one layer model we had previously and interpretability of models is on the forefront of current research. Some ideas are suggested here for you and you should implement all of them, but feel free to go above and beyond to try and understand the model you've trained. If you are interested in more details, take a look at https://transformer-circuits.pub/2023/monosemantic-features/index.html and https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html for interpretability done by Anthropic.